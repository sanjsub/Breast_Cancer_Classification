{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "from skimage import io\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "def setup_seed(seed): \n",
    "    torch.manual_seed(seed) \n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    np.random.seed(seed) \n",
    "    random.seed(seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    return None\n",
    "setup_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device# = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>StudyUID</th>\n",
       "      <th>Label</th>\n",
       "      <th>View</th>\n",
       "      <th>descriptive_path</th>\n",
       "      <th>classic_path</th>\n",
       "      <th>new_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DBT-P02497</td>\n",
       "      <td>DBT-S00143</td>\n",
       "      <td>0</td>\n",
       "      <td>lcc</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02497/1.2.826...</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DBT-P02497</td>\n",
       "      <td>DBT-S00143</td>\n",
       "      <td>0</td>\n",
       "      <td>lmlo</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02497/1.2.826...</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DBT-P02497</td>\n",
       "      <td>DBT-S00143</td>\n",
       "      <td>0</td>\n",
       "      <td>rcc</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02497/1.2.826...</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DBT-P02497</td>\n",
       "      <td>DBT-S00143</td>\n",
       "      <td>0</td>\n",
       "      <td>rmlo</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02497/1.2.826...</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBT-P02449</td>\n",
       "      <td>DBT-S05000</td>\n",
       "      <td>0</td>\n",
       "      <td>lcc</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02449/01-01-2...</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02449/1.2.826...</td>\n",
       "      <td>Breast-Cancer-Screening-DBT/DBT-P02449/01-01-2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PatientID    StudyUID  Label  View  \\\n",
       "0  DBT-P02497  DBT-S00143      0   lcc   \n",
       "1  DBT-P02497  DBT-S00143      0  lmlo   \n",
       "2  DBT-P02497  DBT-S00143      0   rcc   \n",
       "3  DBT-P02497  DBT-S00143      0  rmlo   \n",
       "4  DBT-P02449  DBT-S05000      0   lcc   \n",
       "\n",
       "                                    descriptive_path  \\\n",
       "0  Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...   \n",
       "1  Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...   \n",
       "2  Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...   \n",
       "3  Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...   \n",
       "4  Breast-Cancer-Screening-DBT/DBT-P02449/01-01-2...   \n",
       "\n",
       "                                        classic_path  \\\n",
       "0  Breast-Cancer-Screening-DBT/DBT-P02497/1.2.826...   \n",
       "1  Breast-Cancer-Screening-DBT/DBT-P02497/1.2.826...   \n",
       "2  Breast-Cancer-Screening-DBT/DBT-P02497/1.2.826...   \n",
       "3  Breast-Cancer-Screening-DBT/DBT-P02497/1.2.826...   \n",
       "4  Breast-Cancer-Screening-DBT/DBT-P02449/1.2.826...   \n",
       "\n",
       "                                            new_path  \n",
       "0  Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...  \n",
       "1  Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...  \n",
       "2  Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...  \n",
       "3  Breast-Cancer-Screening-DBT/DBT-P02497/01-01-2...  \n",
       "4  Breast-Cancer-Screening-DBT/DBT-P02449/01-01-2...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('DBT_train_resized.csv', index_col = 0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 12760, 1: 143})\n",
      "Counter({0: 715, 1: 143})\n"
     ]
    }
   ],
   "source": [
    "# Undersample 0 class\n",
    "print(Counter(data.Label))\n",
    "data0 = data[data.Label == 0]\n",
    "data1 = data[data.Label == 1]\n",
    "downsample = data0.sample(n = len(data1)*5)\n",
    "ind = list(downsample.index)\n",
    "ind = ind + list(data1.index)\n",
    "newdf = data[data.index.isin(ind)]\n",
    "print(Counter(newdf.Label))\n",
    "newdf.to_csv('DBT_train_resized_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of biopsied samples: 143\n",
      "Number of normal samples: 12760\n",
      "Disease Ratio: 0.011082693947144074\n"
     ]
    }
   ],
   "source": [
    "# Oversample 1 class\n",
    "print('Number of biopsied samples:', np.sum(data['Label'].values))\n",
    "print('Number of normal samples:', len(data) - np.sum(data['Label'].values))\n",
    "disease_ratio = np.sum(data['Label'].values) / len(data)\n",
    "print('Disease Ratio:', disease_ratio)\n",
    "\n",
    "labels_unique, counts = np.unique(data.Label, return_counts = True)\n",
    "class_weights = [sum(counts)/c for c in counts]\n",
    "example_weights = [class_weights[e] for e in data.Label]\n",
    "sampler = WeightedRandomSampler(example_weights, len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(tl_model, train_scratch=False, freeze_weights=False):\n",
    "    # load the (pretrained) model from torchvision library\n",
    "    if tl_model == \"Resnet18\":\n",
    "        if (train_scratch):\n",
    "            model_ft = models.resnet18(pretrained=False)\n",
    "        else:\n",
    "            model_ft = models.resnet18(pretrained=True)\n",
    "    if tl_model == \"Resnet34\":\n",
    "        if (train_scratch):\n",
    "            model_ft = models.resnet34(pretrained=False)\n",
    "        else:\n",
    "            model_ft = models.resnet34(pretrained=True)\n",
    "    elif tl_model == \"Resnet50\":\n",
    "        if (train_scratch):\n",
    "            model_ft = models.resnet50(pretrained=False)\n",
    "        else: \n",
    "            model_ft = models.resnet50(pretrained=True)\n",
    "    elif tl_model == \"DenseNet201\":\n",
    "        if (train_scratch):\n",
    "            model_ft = models.densenet201(pretrained=False)\n",
    "        else:\n",
    "            model_ft = models.densenet201(pretrained=True)\n",
    "    elif tl_model == \"DenseNet169\":\n",
    "        if (train_scratch):\n",
    "            model_ft = models.densenet169(pretrained=False)\n",
    "        else:\n",
    "            model_ft = models.densenet169(pretrained=True)\n",
    "    elif tl_model == \"DenseNet121\":\n",
    "        if (train_scratch):\n",
    "            model_ft = models.densenet121(pretrained=False)\n",
    "        else:\n",
    "            model_ft = models.densenet121(pretrained=True)\n",
    "    else: \n",
    "        raise ValueError(f'tl_model={tl_model} is not recognized!')\n",
    "\n",
    "    # freeze the weights if necessary\n",
    "    if freeze_weights:\n",
    "        for param in model_ft.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # replace last fc layer with binary classification \n",
    "    if tl_model in (\"DenseNet201\", \"DenseNet169\", \"DenseNet121\"):\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
    "    else:\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        model_ft.fc = nn.Linear(num_ftrs, 2)      \n",
    "\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(tl_model, train_scratch=False, freeze_weights=False):\n",
    "    # load the (pretrained) model from torchvision library\n",
    "    if tl_model == \"resnet18\":\n",
    "        if (train_scratch):\n",
    "            model_ft = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
    "        else:\n",
    "            model_ft = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "    if tl_model == \"resnet34\":\n",
    "        if (train_scratch):\n",
    "            model_ft = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=False)\n",
    "        else:\n",
    "            model_ft = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
    "    elif tl_model == \"resnet50\":\n",
    "        if (train_scratch):\n",
    "            model_ft = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=False)\n",
    "        else:\n",
    "            model_ft = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "    elif tl_model == \"densenet201\":\n",
    "        if (train_scratch):\n",
    "            model_ft = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=False)\n",
    "        else:\n",
    "            model_ft = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)\n",
    "    elif tl_model == \"densenet169\":\n",
    "        if (train_scratch):\n",
    "            model_ft = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=False)\n",
    "        else:\n",
    "            model_ft = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)\n",
    "    elif tl_model == \"densenet121\":\n",
    "        if (train_scratch):\n",
    "            model_ft = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=False)\n",
    "        else:\n",
    "            model_ft = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n",
    "    else: \n",
    "        raise ValueError(f'tl_model={tl_model} is not recognized!')\n",
    "\n",
    "    # freeze the weights if necessary\n",
    "    if freeze_weights:\n",
    "        for param in model_ft.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # replace last fc layer with binary classification \n",
    "    if tl_model in (\"densenet201\", \"densenet169\", \"densenet121\"):\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
    "    else:\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        model_ft.fc = nn.Linear(num_ftrs, 2)      \n",
    "\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                                    #transforms.ToPILImage(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.RandomRotation(20),\n",
    "                                    transforms.RandomResizedCrop(224, scale=(0.8, 1.2)),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ColorJitter(hue=0.5, saturation=0.5, contrast=0.5),\n",
    "                                    transforms.GaussianBlur(7, sigma=(0.1, 1.0)),\n",
    "                                    #transforms.Resize((224,224))\n",
    "                                ])\n",
    "val_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                     #transforms.Resize((224,224))\n",
    "                                     ])\n",
    "\n",
    "class DBT_Dataset(Dataset):\n",
    "    def __init__(self, df_path, train = False):\n",
    "        self.df = pd.read_csv(df_path)\n",
    "        self.train = train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx, -1]\n",
    "        img = io.imread(img_name, as_gray=True)\n",
    "        #img = np.asarray(img)\n",
    "        img.astype(float)\n",
    "        \n",
    "        img = img - np.mean(img)\n",
    "        img = img / np.maximum(np.std(img), 10**(-5))\n",
    "        \n",
    "        if self.train:\n",
    "            img_tens = train_transforms(img)\n",
    "        else:\n",
    "            img_tens = val_transforms(img)\n",
    "            \n",
    "        label = self.df['Label'].iloc[idx]\n",
    "        #label = self.df.loc[idx,'Label'].astype('int')\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        sample = (img_tens.float(), label)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "bs = 16\n",
    "train_df_path = 'DBT_train_resized.csv'\n",
    "val_df_path = 'DBT_val_resized.csv'\n",
    "test_df_path = 'DBT_test_resized.csv'\n",
    "\n",
    "train_loader = DataLoader(DBT_Dataset(train_df_path, train=True), batch_size=bs, num_workers=8, pin_memory=True, sampler=sampler, drop_last=True)\n",
    "val_loader = DataLoader(DBT_Dataset(val_df_path), batch_size=bs, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(DBT_Dataset(test_df_path), batch_size=bs, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ss14383/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "# pull pre-trained model\n",
    "tl_model = \"resnet34\"\n",
    "train_scratch = False\n",
    "freeze_weights = False\n",
    "model = get_model(tl_model, train_scratch, freeze_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other models\n",
    "model = models.vgg16(pretrained=True)\n",
    "model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "num_ftrs = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "lambda_func = lambda epoch: 0.5 ** epoch\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_func)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 30, gamma = 0.1)#lr_lambda=lambda_func,)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "save_path = 'test_model_balanced3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, train_loader, val_loader, learning_rate, optimizer, scheduler, loss_fn, epochs, save_path):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss_return = []\n",
    "    train_acc_return = []\n",
    "    val_loss_return = []\n",
    "    val_acc_return = []\n",
    "    best_acc = -1\n",
    "    # Train\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch: {}/{}'.format(epoch, epochs-1))\n",
    "        print('-'*10)\n",
    "        pred_list = []\n",
    "        pred_scores_list = []\n",
    "        truths_list = []\n",
    "        loss_list = []\n",
    "        model.train()\n",
    "        for idx, (sample) in enumerate(train_loader):\n",
    "            img = sample[0].to(device)\n",
    "            labels = sample[1].squeeze(0).to(device)\n",
    "            outputs = model(img)\n",
    "            model.zero_grad()\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_list.append(loss.item())\n",
    "            pred_score = nn.Softmax(1)(outputs).cpu().detach().numpy()\n",
    "            pred_scores_list += pred_score.tolist()\n",
    "            pred  = np.argmax(pred_score,axis=1)\n",
    "            pred_list += pred.tolist()\n",
    "            truths_list += labels.cpu().numpy().tolist()\n",
    "        scheduler.step(metrics = loss)\n",
    "        # report performance\n",
    "        correct_num = (np.array(pred_list) == np.array(truths_list)).sum()\n",
    "        acc = correct_num/len(truths_list)\n",
    "        train_acc_return.append(acc)\n",
    "        train_loss_return.append(np.average(loss_list))\n",
    "        print('----------Epoch{:2d}/{:2d}----------'.format(epoch+1, epochs))\n",
    "        print('Train set | Loss: {:6.4f} | Accuracy: {:4.2f}% '.format(np.average(loss_list), acc*100))\n",
    "        \n",
    "        # Val\n",
    "        pred_list = []\n",
    "        pred_scores_list = []\n",
    "        truths_list = []\n",
    "        loss_list = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, (sample) in enumerate(val_loader):\n",
    "                img = sample[0].to(device)\n",
    "                labels = sample[1].squeeze(0).to(device)\n",
    "                outputs = model(img)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss_list.append(loss.item())\n",
    "                pred_score = nn.Softmax(1)(outputs).cpu().detach().numpy()\n",
    "                pred_scores_list += pred_score.tolist()\n",
    "                pred  = np.argmax(pred_score,axis=1)\n",
    "                pred_list += pred.tolist()\n",
    "                truths_list += labels.cpu().numpy().tolist()\n",
    "            # report performance\n",
    "            correct_num = (np.array(pred_list) == np.array(truths_list)).sum()\n",
    "            acc = correct_num/len(truths_list)\n",
    "            val_acc_return.append(acc)\n",
    "            val_loss_return.append(np.average(loss_list))\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_model_wts = model.state_dict()\n",
    "            elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "            print('Val set  | Loss: {:6.4f} | Accuracy: {:4.2f}% | Best ACC: {:6.4f} | time elapse: {:>9}'\\\n",
    "                  .format(np.average(loss_list), acc*100, best_acc*100, elapse))\n",
    "            save_model(model, best_model_wts, train_loss_return, train_acc_return,\\\n",
    "                       val_loss_return, val_acc_return, save_path=save_path)\n",
    "            \n",
    "    return None\n",
    "\n",
    "def save_model(model, best_model_wts, train_loss_return,train_acc_return,\\\n",
    "               val_loss_return, val_acc_return, save_path):\n",
    "    state = {'best_model_wts':best_model_wts, 'model':model, \\\n",
    "             'train_loss':train_loss_return, 'train_acc':train_acc_return,\\\n",
    "             'val_loss':val_loss_return, 'val_acc':val_acc_return}\n",
    "    torch.save(state, save_path+'.pt')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/9\n",
      "----------\n",
      "----------Epoch 1/10----------\n",
      "Train set | Loss: 0.0799 | Accuracy: 98.87% \n",
      "Val set  | Loss: 0.0691 | Accuracy: 98.97% | Best ACC: 98.9685 | time elapse:  00:12:22\n",
      "Epoch: 1/9\n",
      "----------\n",
      "----------Epoch 2/10----------\n",
      "Train set | Loss: 0.0657 | Accuracy: 98.89% \n",
      "Val set  | Loss: 0.1037 | Accuracy: 98.97% | Best ACC: 98.9685 | time elapse:  00:15:12\n",
      "Epoch: 2/9\n",
      "----------\n",
      "----------Epoch 3/10----------\n",
      "Train set | Loss: 0.0654 | Accuracy: 98.89% \n",
      "Val set  | Loss: 0.0794 | Accuracy: 98.97% | Best ACC: 98.9685 | time elapse:  00:18:19\n",
      "Epoch: 3/9\n",
      "----------\n",
      "----------Epoch 4/10----------\n",
      "Train set | Loss: 0.0651 | Accuracy: 98.89% \n",
      "Val set  | Loss: 0.0785 | Accuracy: 98.97% | Best ACC: 98.9685 | time elapse:  00:20:56\n",
      "Epoch: 4/9\n",
      "----------\n",
      "----------Epoch 5/10----------\n",
      "Train set | Loss: 0.0636 | Accuracy: 98.89% \n",
      "Val set  | Loss: 0.0744 | Accuracy: 98.97% | Best ACC: 98.9685 | time elapse:  00:23:32\n",
      "Epoch: 5/9\n",
      "----------\n",
      "----------Epoch 6/10----------\n",
      "Train set | Loss: 0.0621 | Accuracy: 98.89% \n",
      "Val set  | Loss: 0.0615 | Accuracy: 98.97% | Best ACC: 98.9685 | time elapse:  00:26:11\n",
      "Epoch: 6/9\n",
      "----------\n",
      "----------Epoch 7/10----------\n",
      "Train set | Loss: 0.0620 | Accuracy: 98.89% \n",
      "Val set  | Loss: 0.0652 | Accuracy: 98.97% | Best ACC: 98.9685 | time elapse:  00:28:47\n",
      "Epoch: 7/9\n",
      "----------\n",
      "----------Epoch 8/10----------\n",
      "Train set | Loss: 0.0615 | Accuracy: 98.89% \n",
      "Val set  | Loss: 0.0590 | Accuracy: 98.97% | Best ACC: 98.9685 | time elapse:  00:31:23\n",
      "Epoch: 8/9\n",
      "----------\n",
      "----------Epoch 9/10----------\n",
      "Train set | Loss: 0.0610 | Accuracy: 98.89% \n",
      "Val set  | Loss: 0.0593 | Accuracy: 98.97% | Best ACC: 98.9685 | time elapse:  00:34:01\n",
      "Epoch: 9/9\n",
      "----------\n",
      "----------Epoch10/10----------\n",
      "Train set | Loss: 0.0608 | Accuracy: 98.89% \n",
      "Val set  | Loss: 0.0584 | Accuracy: 98.97% | Best ACC: 98.9685 | time elapse:  00:36:39\n"
     ]
    }
   ],
   "source": [
    "setup_seed(0)\n",
    "model = model.to(device)\n",
    "model_train(model, train_loader, val_loader, learning_rate, optimizer, scheduler, loss_fn, epochs, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/9\n",
      "----------\n",
      "----------Epoch 1/10----------\n",
      "Train set | Loss: 0.6929 | Accuracy: 51.47% \n",
      "Val set  | Loss: 0.6963 | Accuracy: 36.25% | Best ACC: 36.2500 | time elapse:  00:00:53\n",
      "Epoch: 1/9\n",
      "----------\n",
      "----------Epoch 2/10----------\n",
      "Train set | Loss: 0.6928 | Accuracy: 51.84% \n",
      "Val set  | Loss: 0.6933 | Accuracy: 45.60% | Best ACC: 45.5978 | time elapse:  00:01:54\n",
      "Epoch: 2/9\n",
      "----------\n",
      "----------Epoch 3/10----------\n",
      "Train set | Loss: 0.6927 | Accuracy: 51.81% \n",
      "Val set  | Loss: 0.6945 | Accuracy: 42.09% | Best ACC: 45.5978 | time elapse:  00:02:49\n",
      "Epoch: 3/9\n",
      "----------\n",
      "----------Epoch 4/10----------\n",
      "Train set | Loss: 0.6926 | Accuracy: 51.93% \n",
      "Val set  | Loss: 0.6868 | Accuracy: 63.23% | Best ACC: 63.2337 | time elapse:  00:03:44\n",
      "Epoch: 4/9\n",
      "----------\n",
      "----------Epoch 5/10----------\n",
      "Train set | Loss: 0.6924 | Accuracy: 52.62% \n",
      "Val set  | Loss: 0.6899 | Accuracy: 54.24% | Best ACC: 63.2337 | time elapse:  00:04:46\n",
      "Epoch: 5/9\n",
      "----------\n",
      "----------Epoch 6/10----------\n",
      "Train set | Loss: 0.6926 | Accuracy: 51.78% \n",
      "Val set  | Loss: 0.6877 | Accuracy: 59.84% | Best ACC: 63.2337 | time elapse:  00:05:49\n",
      "Epoch: 6/9\n",
      "----------\n",
      "----------Epoch 7/10----------\n",
      "Train set | Loss: 0.6924 | Accuracy: 53.06% \n",
      "Val set  | Loss: 0.6873 | Accuracy: 60.35% | Best ACC: 63.2337 | time elapse:  00:06:53\n",
      "Epoch: 7/9\n",
      "----------\n",
      "----------Epoch 8/10----------\n",
      "Train set | Loss: 0.6924 | Accuracy: 52.55% \n",
      "Val set  | Loss: 0.6873 | Accuracy: 61.39% | Best ACC: 63.2337 | time elapse:  00:07:57\n",
      "Epoch: 8/9\n",
      "----------\n",
      "----------Epoch 9/10----------\n",
      "Train set | Loss: 0.6928 | Accuracy: 51.54% \n",
      "Val set  | Loss: 0.6867 | Accuracy: 61.60% | Best ACC: 63.2337 | time elapse:  00:08:54\n",
      "Epoch: 9/9\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "setup_seed(0)\n",
    "model = model.to(device)\n",
    "model_train(model, train_loader, val_loader, learning_rate, optimizer, scheduler, loss_fn, epochs, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best weights\n",
    "path = 'test_model_balanced2.pt'\n",
    "model.load_state_dict(torch.load(path)['best_model_wts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def model_eval(model, test_loader):\n",
    "    pred_list = []\n",
    "    pred_scores_list = []\n",
    "    truths_list = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (sample) in enumerate(test_loader):\n",
    "            img = sample[0].to(device)\n",
    "            labels = sample[1].to(device)#.squeeze(1).to(device)\n",
    "            outputs = model(img)\n",
    "            pred_score = nn.Softmax(1)(outputs).cpu().detach().numpy()\n",
    "            pred_scores_list += pred_score.tolist()\n",
    "            pred  = np.argmax(pred_score,axis=1)\n",
    "            pred_list += pred.tolist()\n",
    "            truths_list += labels.cpu().numpy().tolist()\n",
    "    return np.array(pred_list), np.array(pred_scores_list), np.array(truths_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, pred_scores, truths = model_eval(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prediction:',pred[:10])\n",
    "print('Truth:     ',truths[:10])\n",
    "print('Prediction Probability:\\n',pred_scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve(y_test, y_score):\n",
    "    fpr, tpr, roc_auc = {},{},{}\n",
    "    n_classes = y_test.shape[1]\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange'])\n",
    "    lw = 2\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_is_0 = (truths == 0).astype(int)\n",
    "truth_is_1 = (truths == 1).astype(int)\n",
    "y_test = np.array([truth_is_0, truth_is_1])\n",
    "y_test = np.transpose(y_test,(1,0))\n",
    "print('Truth = 0:',truth_is_0[:10])\n",
    "print('Truth = 1:',truth_is_1[:10])\n",
    "print('Label for ROC:')\n",
    "print(y_test[:,:10])\n",
    "print('Prediction Probability for ROC:')\n",
    "print(pred_scores[:,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_curve(y_test,pred_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(truths,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Normal','Disease']\n",
    "plot_confusion_matrix(truths, pred, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmsc",
   "language": "python",
   "name": "bmsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
